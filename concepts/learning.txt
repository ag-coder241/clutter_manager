
1. Recursive directory scanning & symlink safety - 
a symbolic link is a file that points to another path
rule - never recurse into directory symlinks
detect file symlinks and we will include them in meta deta but we will not follow them as it can 
lead to duplicate counting.

every file system object has inode. the inode stores (type, permissions, timestamps etc).
stat(path) = follows symlink and gives info about path
lstat(path) = doesn't follow the link, only gives info about the symlink node itself

scanning logic - 
If symlink:
    record it
    do NOT recurse
Else if directory:
    // directory and not a symlink
    record it
    recurse
Else:
    record file

Hidden File - any file that begins with "."

type of files - 
1. user files
2. metadata files
3. system files

at the analysis level we will classify the files into categories.

2. Filesystem permissions & access errors - 

if directory level failure - skip the directory
if file level failure - skip only that file

try open directory
    for each entry
        try read metadata
            record file
        catch
            skip this entry
catch
    skip directory

3. Performance and Scalability - 
 ignore specific directory
 incremental scanning - Only process files that have changed since the last scan

 SQLite Schema
 currently - (path (PRIMARY KEY), size, last_modified(integer), is_directory, is_symlink, last_scanner(integer))
 using recursive scanning 
 incremental scanning - using UPSERT(insert, update, skip)

4. controller
 coordinates between scanner and the database
 
5. deletion detection
using last_scanned metric in our database
at start of scan = scan_start_time
during scan = we update last_scan

if after scan last_scanned < scan_start time this file was deleted
we first mark it.

6. ANALYSER

old files
large files 
fetch data from the database in the form of FileInfo and pass to the analysis layer

further analysis heuristic - 
clutter_score = size * age * unused_factor

Unused files (highest priority)
old files (exits and not unused)
large files (exceed a limit)

file type awareness = we should be able to analyse basis on the file type we give 
duplicate file detection

CLI enhancements(basic)

Design decisions doc 
Architecture doc
README 
